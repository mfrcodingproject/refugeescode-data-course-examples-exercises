{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcb93c42",
   "metadata": {},
   "source": [
    "# Introduction to Pandas\n",
    "\n",
    "Pandas is the most popular Python library for data analysis and manipulation. It provides high-performance, easy-to-use data structures and data analysis tools built on top of NumPy.\n",
    "\n",
    "The name \"pandas\" is derived from \"panel data\" - an econometrics term for multidimensional structured datasets.\n",
    "\n",
    "**Official Documentation:** https://pandas.pydata.org/docs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc4b4a2",
   "metadata": {},
   "source": [
    "## Why Pandas?\n",
    "\n",
    "### Advantages:\n",
    "1. **Easy Data Handling**: Work with structured data intuitively\n",
    "2. **Data Cleaning**: Handle missing data, duplicates, and inconsistencies\n",
    "3. **Data Transformation**: Reshape, merge, join, and pivot data easily\n",
    "4. **Time Series**: Built-in support for time series data\n",
    "5. **Integration**: Works seamlessly with NumPy, Matplotlib, and other libraries\n",
    "6. **I/O Operations**: Read/write CSV, Excel, SQL, JSON, and more\n",
    "\n",
    "### Key Features:\n",
    "- DataFrame and Series data structures\n",
    "- Intelligent data alignment\n",
    "- Flexible grouping and aggregation\n",
    "- Built-in visualization\n",
    "- Efficient indexing and selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3511fcad",
   "metadata": {},
   "source": [
    "## When to Use Pandas vs NumPy\n",
    "\n",
    "Understanding when to use each library is important for efficient data analysis:\n",
    "\n",
    "### Use NumPy when:\n",
    "- Working with numerical arrays and matrices\n",
    "- Need fast mathematical operations\n",
    "- Data is homogeneous (all same type)\n",
    "- Working with multi-dimensional numerical data\n",
    "- Memory efficiency is critical\n",
    "\n",
    "### Use Pandas when:\n",
    "- Working with tabular data (rows and columns)\n",
    "- Need to handle mixed data types\n",
    "- Working with labeled data (column names, indices)\n",
    "- Need data cleaning and transformation tools\n",
    "- Reading/writing data from files (CSV, Excel, SQL)\n",
    "- Performing group-by operations and aggregations\n",
    "- Working with time series data\n",
    "\n",
    "### Best Practice:\n",
    "**Use both together!** Pandas is built on top of NumPy, and you can easily convert between them:\n",
    "- DataFrame → NumPy: `df.values` or `df.to_numpy()`\n",
    "- NumPy → DataFrame: `pd.DataFrame(array)`\n",
    "\n",
    "**Example workflow:** Load data with Pandas → Clean with Pandas → Convert to NumPy for numerical operations → Convert back to Pandas for results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbf1ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Example: Pandas for structured data\n",
    "df = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'Age': [25, 30, 35],\n",
    "    'Salary': [50000, 60000, 55000]\n",
    "})\n",
    "print(\"Pandas DataFrame (mixed types):\")\n",
    "print(df)\n",
    "print(\"\\nData types:\", df.dtypes.tolist())\n",
    "\n",
    "# Convert to NumPy for numerical operations\n",
    "ages_array = df['Age'].values\n",
    "salaries_array = df['Salary'].values\n",
    "\n",
    "print(\"\\nNumPy arrays (numerical operations):\")\n",
    "print(\"Ages:\", ages_array)\n",
    "print(\"Salaries:\", salaries_array)\n",
    "\n",
    "# Perform NumPy operations\n",
    "age_mean = np.mean(ages_array)\n",
    "salary_normalized = (salaries_array - np.mean(salaries_array)) / np.std(salaries_array)\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "print(f\"Average age: {age_mean}\")\n",
    "print(f\"Normalized salaries: {salary_normalized}\")\n",
    "\n",
    "# Convert back to Pandas\n",
    "df['Salary_Normalized'] = salary_normalized\n",
    "print(\"\\nBack to Pandas with new column:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54dcf07",
   "metadata": {},
   "source": [
    "## Core Data Structures\n",
    "\n",
    "Pandas has two main data structures:\n",
    "\n",
    "1. **Series**: One-dimensional labeled array (like a column)\n",
    "2. **DataFrame**: Two-dimensional labeled data structure (like a table)\n",
    "\n",
    "**Documentation:** https://pandas.pydata.org/docs/user_guide/dsintro.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ebf1f7",
   "metadata": {},
   "source": [
    "## Pandas Series\n",
    "\n",
    "A Series is a one-dimensional array with labels (index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c60088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Series from a list\n",
    "series_from_list = pd.Series([10, 20, 30, 40, 50])\n",
    "print(\"Series from list:\")\n",
    "print(series_from_list)\n",
    "print(\"\\nData type:\", type(series_from_list))\n",
    "\n",
    "# Create a Series with custom index\n",
    "series_with_index = pd.Series([10, 20, 30, 40, 50], \n",
    "                               index=['a', 'b', 'c', 'd', 'e'])\n",
    "print(\"\\nSeries with custom index:\")\n",
    "print(series_with_index)\n",
    "\n",
    "# Create a Series from a dictionary\n",
    "data_dict = {'apple': 5, 'banana': 3, 'orange': 8}\n",
    "series_from_dict = pd.Series(data_dict)\n",
    "print(\"\\nSeries from dictionary:\")\n",
    "print(series_from_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697bff55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Series attributes and methods\n",
    "series = pd.Series([10, 20, 30, 40, 50], index=['a', 'b', 'c', 'd', 'e'])\n",
    "\n",
    "print(\"Values:\", series.values)\n",
    "print(\"Index:\", series.index)\n",
    "print(\"Shape:\", series.shape)\n",
    "print(\"Size:\", series.size)\n",
    "print(\"Data type:\", series.dtype)\n",
    "\n",
    "# Accessing elements\n",
    "print(\"\\nAccess by index label:\", series['c'])\n",
    "print(\"Access by position:\", series[2])\n",
    "print(\"Access multiple:\", series[['a', 'c', 'e']])\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nMean:\", series.mean())\n",
    "print(\"Sum:\", series.sum())\n",
    "print(\"Max:\", series.max())\n",
    "print(\"Min:\", series.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5824fb9",
   "metadata": {},
   "source": [
    "## Pandas DataFrame\n",
    "\n",
    "A DataFrame is a 2-dimensional labeled data structure with columns of potentially different types. Think of it as a spreadsheet or SQL table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cc66dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame from dictionary\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "    'Age': [25, 30, 35, 28, 32],\n",
    "    'City': ['New York', 'London', 'Paris', 'Tokyo', 'Berlin'],\n",
    "    'Salary': [50000, 60000, 55000, 65000, 58000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"DataFrame from dictionary:\")\n",
    "print(df)\n",
    "\n",
    "# Create DataFrame from list of lists\n",
    "data_list = [\n",
    "    ['Alice', 25, 'New York', 50000],\n",
    "    ['Bob', 30, 'London', 60000],\n",
    "    ['Charlie', 35, 'Paris', 55000]\n",
    "]\n",
    "df_from_list = pd.DataFrame(data_list, \n",
    "                             columns=['Name', 'Age', 'City', 'Salary'])\n",
    "print(\"\\nDataFrame from list:\")\n",
    "print(df_from_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff127dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame attributes and methods\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"\\nColumn names:\", df.columns.tolist())\n",
    "print(\"\\nIndex:\", df.index.tolist())\n",
    "print(\"\\nData types:\\n\", df.dtypes)\n",
    "print(\"\\nInfo:\")\n",
    "df.info()\n",
    "print(\"\\nFirst 3 rows:\")\n",
    "print(df.head(3))\n",
    "print(\"\\nLast 2 rows:\")\n",
    "print(df.tail(2))\n",
    "print(\"\\nDescriptive statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f5ffc6",
   "metadata": {},
   "source": [
    "## Selecting Data\n",
    "\n",
    "Pandas provides multiple ways to select and access data:\n",
    "\n",
    "**Documentation:** https://pandas.pydata.org/docs/user_guide/indexing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc7d807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "    'Age': [25, 30, 35, 28, 32],\n",
    "    'City': ['New York', 'London', 'Paris', 'Tokyo', 'Berlin'],\n",
    "    'Salary': [50000, 60000, 55000, 65000, 58000]\n",
    "})\n",
    "\n",
    "# Select single column (returns Series)\n",
    "print(\"Select 'Name' column:\")\n",
    "print(df['Name'])\n",
    "print(\"\\nType:\", type(df['Name']))\n",
    "\n",
    "# Select multiple columns (returns DataFrame)\n",
    "print(\"\\nSelect multiple columns:\")\n",
    "print(df[['Name', 'Age']])\n",
    "\n",
    "# Select rows by index position (iloc)\n",
    "print(\"\\nFirst row (iloc):\")\n",
    "print(df.iloc[0])\n",
    "\n",
    "print(\"\\nFirst 3 rows:\")\n",
    "print(df.iloc[0:3])\n",
    "\n",
    "# Select rows by index label (loc)\n",
    "print(\"\\nSelect by label (loc):\")\n",
    "print(df.loc[1:3, ['Name', 'City']])\n",
    "\n",
    "# Select specific cells\n",
    "print(\"\\nSelect specific cell:\")\n",
    "print(df.loc[2, 'Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776e1eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean indexing (filtering)\n",
    "print(\"People older than 30:\")\n",
    "print(df[df['Age'] > 30])\n",
    "\n",
    "print(\"\\nPeople in New York or London:\")\n",
    "print(df[df['City'].isin(['New York', 'London'])])\n",
    "\n",
    "# Multiple conditions (AND)\n",
    "print(\"\\nAge > 25 AND Salary > 55000:\")\n",
    "print(df[(df['Age'] > 25) & (df['Salary'] > 55000)])\n",
    "\n",
    "# Multiple conditions (OR)\n",
    "print(\"\\nAge < 30 OR Salary > 60000:\")\n",
    "print(df[(df['Age'] < 30) | (df['Salary'] > 60000)])\n",
    "\n",
    "# String methods\n",
    "print(\"\\nNames starting with 'A' or 'B':\")\n",
    "print(df[df['Name'].str.startswith(('A', 'B'))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6228737e",
   "metadata": {},
   "source": [
    "## Understanding loc vs iloc vs [] - Deep Dive\n",
    "\n",
    "One of the most confusing aspects of Pandas for beginners is understanding the difference between `loc`, `iloc`, and bracket indexing `[]`. Let's clarify this once and for all:\n",
    "\n",
    "### The Three Ways to Select Data:\n",
    "\n",
    "1. **`[]` (bracket notation)**: Mixed behavior, can be confusing\n",
    "2. **`.loc[]`**: Label-based indexing (explicit index)\n",
    "3. **`.iloc[]`**: Position-based indexing (integer position)\n",
    "\n",
    "**Best Practice:** Always use `.loc[]` or `.iloc[]` to be explicit and avoid confusion!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b5e707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with INTEGER index to show the confusion\n",
    "df = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "    'Age': [25, 30, 35, 28, 32],\n",
    "    'City': ['NYC', 'London', 'Paris', 'Tokyo', 'Berlin']\n",
    "}, index=[10, 20, 30, 40, 50])  # Non-sequential integer index!\n",
    "\n",
    "print(\"DataFrame with custom integer index:\")\n",
    "print(df)\n",
    "print(\"\\nIndex:\", df.index.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a4510b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE CONFUSION: What does df[10] mean?\n",
    "# Is it position 10 or label 10?\n",
    "\n",
    "# Using bracket notation with integer index\n",
    "try:\n",
    "    print(\"df[10] - ERROR! Tries to use position 10 (doesn't exist)\")\n",
    "    print(df[10])\n",
    "except KeyError as e:\n",
    "    print(f\"KeyError: {e}\")\n",
    "\n",
    "# Slicing with brackets uses POSITION (implicit index)\n",
    "print(\"\\ndf[0:2] - Uses POSITION (first 2 rows):\")\n",
    "print(df[0:2])\n",
    "\n",
    "# But selecting a single value would try to use LABEL\n",
    "# This is CONFUSING!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48e584e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION 1: Use .loc[] for LABEL-based selection\n",
    "print(\"Using .loc[] - LABEL-based (uses index values):\")\n",
    "print(\"\\ndf.loc[10] - Gets row with index label 10:\")\n",
    "print(df.loc[10])\n",
    "\n",
    "print(\"\\ndf.loc[10:30] - Slice by labels (INCLUDES endpoint!):\")\n",
    "print(df.loc[10:30])\n",
    "\n",
    "print(\"\\ndf.loc[10, 'Name'] - Get specific cell:\")\n",
    "print(df.loc[10, 'Name'])\n",
    "\n",
    "print(\"\\ndf.loc[[10, 30, 50], ['Name', 'Age']] - Multiple rows and columns:\")\n",
    "print(df.loc[[10, 30, 50], ['Name', 'Age']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ce2ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION 2: Use .iloc[] for POSITION-based selection\n",
    "print(\"Using .iloc[] - POSITION-based (like NumPy arrays):\")\n",
    "print(\"\\ndf.iloc[0] - Gets FIRST row (position 0):\")\n",
    "print(df.iloc[0])\n",
    "\n",
    "print(\"\\ndf.iloc[0:2] - Slice by position (EXCLUDES endpoint):\")\n",
    "print(df.iloc[0:2])\n",
    "\n",
    "print(\"\\ndf.iloc[0, 1] - Get cell at position [0, 1]:\")\n",
    "print(df.iloc[0, 1])\n",
    "\n",
    "print(\"\\ndf.iloc[[0, 2, 4], [0, 1]] - Multiple positions:\")\n",
    "print(df.iloc[[0, 2, 4], [0, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd775f0d",
   "metadata": {},
   "source": [
    "### Key Differences Summary:\n",
    "\n",
    "| Feature | `.loc[]` | `.iloc[]` | `[]` |\n",
    "|---------|----------|-----------|------|\n",
    "| **Selection Type** | Label-based | Position-based | Mixed (confusing!) |\n",
    "| **Slicing Endpoint** | **Included** | **Excluded** | Excluded |\n",
    "| **Works with** | Index labels | Integer positions | Depends |\n",
    "| **Single value** | `df.loc[label]` | `df.iloc[pos]` | `df[col]` (column only) |\n",
    "| **Slicing** | `df.loc[10:30]` | `df.iloc[0:2]` | `df[0:2]` (position) |\n",
    "| **Recommended?** | ✅ Yes, explicit | ✅ Yes, explicit | ⚠️ Use with caution |\n",
    "\n",
    "### The Golden Rule:\n",
    "\n",
    "**Always prefer `.loc[]` or `.iloc[]` over `[]` for row selection!**\n",
    "\n",
    "- Use `.loc[]` when you know the index labels\n",
    "- Use `.iloc[]` when you care about positions\n",
    "- Use `[]` only for column selection: `df['column']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a03f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical example: Why explicit is better\n",
    "\n",
    "# Bad practice (confusing):\n",
    "df_subset = df[0:2]  # Uses position\n",
    "# df_value = df[10]  # Would cause error!\n",
    "\n",
    "# Good practice (explicit and clear):\n",
    "df_subset_loc = df.loc[10:30]  # Clear: using labels\n",
    "df_subset_iloc = df.iloc[0:2]  # Clear: using positions\n",
    "\n",
    "print(\"Using .loc[] (labels 10:30, includes 30):\")\n",
    "print(df_subset_loc)\n",
    "\n",
    "print(\"\\nUsing .iloc[] (positions 0:2, excludes 2):\")\n",
    "print(df_subset_iloc)\n",
    "\n",
    "# Both are explicit about what they're doing!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2807c85",
   "metadata": {},
   "source": [
    "## Handling Missing Data\n",
    "\n",
    "Real-world data often contains missing values. Pandas provides tools to handle them:\n",
    "\n",
    "**Documentation:** https://pandas.pydata.org/docs/user_guide/missing_data.html\n",
    "\n",
    "### Understanding None vs NaN\n",
    "\n",
    "Pandas uses two Python values to represent missing data:\n",
    "- **`None`**: A Python singleton object (used in object arrays)\n",
    "- **`NaN`**: \"Not a Number\", a special floating-point value (used in numeric arrays)\n",
    "\n",
    "**Important:** Pandas treats both as essentially interchangeable for indicating missing values, but there are key differences under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea42c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding None vs NaN\n",
    "\n",
    "# None - Python object\n",
    "vals_none = pd.Series([1, None, 3, 4])\n",
    "print(\"Series with None:\")\n",
    "print(vals_none)\n",
    "print(\"Data type:\", vals_none.dtype)  # object dtype\n",
    "\n",
    "# NaN - Numeric missing value\n",
    "vals_nan = pd.Series([1, np.nan, 3, 4])\n",
    "print(\"\\nSeries with NaN:\")\n",
    "print(vals_nan)\n",
    "print(\"Data type:\", vals_nan.dtype)  # float64 dtype\n",
    "\n",
    "# Pandas converts between them automatically\n",
    "print(\"\\nAutomatic conversion:\")\n",
    "print(\"None in numeric context becomes NaN:\", vals_none.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d29eaa",
   "metadata": {},
   "source": [
    "### Type Conversion with Missing Values\n",
    "\n",
    "When you introduce missing values, Pandas may automatically convert data types:\n",
    "\n",
    "| Original Type | Missing Value Added | Resulting Type | NA Representation |\n",
    "|--------------|---------------------|----------------|-------------------|\n",
    "| `int` | `np.nan` or `None` | `float64` | `np.nan` |\n",
    "| `float` | `np.nan` or `None` | `float64` | `np.nan` |\n",
    "| `bool` | `np.nan` or `None` | `object` | `None` or `np.nan` |\n",
    "| `object` | `np.nan` or `None` | `object` | `None` or `np.nan` |\n",
    "\n",
    "**Key Point:** Integer arrays are converted to float when NaN is introduced because there's no \"integer NaN\" in NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a0791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type conversion example\n",
    "original = pd.Series([1, 2, 3, 4], dtype='int64')\n",
    "print(\"Original integer Series:\")\n",
    "print(original)\n",
    "print(\"Data type:\", original.dtype)\n",
    "\n",
    "# Add a missing value\n",
    "original_with_nan = original.copy()\n",
    "original_with_nan[2] = np.nan\n",
    "print(\"\\nAfter adding NaN:\")\n",
    "print(original_with_nan)\n",
    "print(\"Data type:\", original_with_nan.dtype)  # Converted to float64!\n",
    "\n",
    "# Boolean conversion\n",
    "bool_series = pd.Series([True, False, True])\n",
    "print(\"\\nOriginal boolean Series:\")\n",
    "print(bool_series)\n",
    "print(\"Data type:\", bool_series.dtype)\n",
    "\n",
    "bool_series[1] = None\n",
    "print(\"\\nAfter adding None:\")\n",
    "print(bool_series)\n",
    "print(\"Data type:\", bool_series.dtype)  # Converted to object!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86207b61",
   "metadata": {},
   "source": [
    "### Detecting Missing Values\n",
    "\n",
    "Use `isnull()` or `notnull()` to detect missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879cd59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with missing values\n",
    "df = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "    'Age': [25, np.nan, 35, 28, np.nan],\n",
    "    'City': ['New York', 'London', None, 'Tokyo', 'Berlin'],\n",
    "    'Salary': [50000, 60000, None, 65000, 58000]\n",
    "})\n",
    "\n",
    "print(\"DataFrame with missing values:\")\n",
    "print(df)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\nAny missing values?:\", df.isnull().any().any())\n",
    "\n",
    "# Visualize missing data\n",
    "print(\"\\nMissing data mask:\")\n",
    "print(df.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a24f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using boolean masks to filter\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Get rows with NO missing values\n",
    "print(\"\\nRows with no missing values:\")\n",
    "print(df[df.notnull().all(axis=1)])\n",
    "\n",
    "# Get rows with ANY missing values\n",
    "print(\"\\nRows with any missing values:\")\n",
    "print(df[df.isnull().any(axis=1)])\n",
    "\n",
    "# Get non-null values from a specific column\n",
    "print(\"\\nNon-null Ages:\")\n",
    "print(df[df['Age'].notnull()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cceb12",
   "metadata": {},
   "source": [
    "### Dropping Missing Values\n",
    "\n",
    "The `dropna()` method removes rows or columns with missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315e7e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with any missing values\n",
    "df_dropped = df.dropna()\n",
    "print(\"After dropping rows with NaN:\")\n",
    "print(df_dropped)\n",
    "\n",
    "# Drop columns with any missing values\n",
    "df_dropped_cols = df.dropna(axis=1)\n",
    "print(\"\\nAfter dropping columns with NaN:\")\n",
    "print(df_dropped_cols)\n",
    "\n",
    "# Drop rows only if all values are missing\n",
    "df_dropped_all = df.dropna(how='all')\n",
    "print(\"\\nAfter dropping rows where all values are NaN:\")\n",
    "print(df_dropped_all)\n",
    "\n",
    "# Drop rows with missing values in specific columns\n",
    "df_dropped_subset = df.dropna(subset=['Age'])\n",
    "print(\"\\nAfter dropping rows with NaN in 'Age':\")\n",
    "print(df_dropped_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2812ed9c",
   "metadata": {},
   "source": [
    "### Advanced dropna(): Using thresh Parameter\n",
    "\n",
    "The `thresh` parameter is very useful in practice - it specifies the **minimum number of non-null values required** to keep a row/column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8f9441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with varying amounts of missing data\n",
    "df_messy = pd.DataFrame({\n",
    "    'A': [1, 2, np.nan, np.nan, 5],\n",
    "    'B': [np.nan, 2, 3, np.nan, 5],\n",
    "    'C': [1, np.nan, np.nan, 4, 5],\n",
    "    'D': [1, 2, 3, 4, 5]\n",
    "})\n",
    "\n",
    "print(\"Messy DataFrame:\")\n",
    "print(df_messy)\n",
    "print(\"\\nMissing values per row:\")\n",
    "print(df_messy.isnull().sum(axis=1))\n",
    "\n",
    "# Keep rows with at least 3 non-null values\n",
    "df_thresh = df_messy.dropna(thresh=3)\n",
    "print(\"\\nKeep rows with at least 3 non-null values:\")\n",
    "print(df_thresh)\n",
    "\n",
    "# Keep rows with at least 75% non-null values\n",
    "min_count = int(len(df_messy.columns) * 0.75)\n",
    "df_percent = df_messy.dropna(thresh=min_count)\n",
    "print(f\"\\nKeep rows with at least {min_count} non-null values (75% of {len(df_messy.columns)} columns):\")\n",
    "print(df_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479de9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresh with axis=1 (columns)\n",
    "print(\"Original DataFrame:\")\n",
    "print(df_messy)\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df_messy.isnull().sum())\n",
    "\n",
    "# Keep columns with at least 4 non-null values\n",
    "df_thresh_cols = df_messy.dropna(axis=1, thresh=4)\n",
    "print(\"\\nKeep columns with at least 4 non-null values:\")\n",
    "print(df_thresh_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fcdde2",
   "metadata": {},
   "source": [
    "### Filling Missing Values\n",
    "\n",
    "Instead of dropping, you can fill missing values with `fillna()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f048509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data for filling examples\n",
    "df_fill = pd.DataFrame({\n",
    "    'A': [1, np.nan, 3, np.nan, 5],\n",
    "    'B': [np.nan, 2, np.nan, 4, 5],\n",
    "    'C': [1, 2, 3, 4, 5]\n",
    "})\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df_fill)\n",
    "\n",
    "# Fill missing values with a constant\n",
    "df_filled = df_fill.fillna(0)\n",
    "print(\"\\nFill NaN with 0:\")\n",
    "print(df_filled)\n",
    "\n",
    "# Fill with different values per column\n",
    "df_filled_dict = df_fill.fillna({'A': df_fill['A'].mean(), \n",
    "                                 'B': df_fill['B'].median()})\n",
    "print(\"\\nFill with different values per column:\")\n",
    "print(df_filled_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999e66e3",
   "metadata": {},
   "source": [
    "### Forward Fill and Backward Fill\n",
    "\n",
    "For time series or ordered data, you can propagate values forward or backward:\n",
    "\n",
    "- **Forward fill (`ffill`)**: Use the last valid value\n",
    "- **Backward fill (`bfill`)**: Use the next valid value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fe4372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Series to demonstrate fill methods clearly\n",
    "data = pd.Series([1, np.nan, np.nan, 4, np.nan, np.nan, 7], \n",
    "                 index=['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
    "\n",
    "print(\"Original Series:\")\n",
    "print(data)\n",
    "\n",
    "# Forward fill - propagate last valid value forward\n",
    "print(\"\\nForward fill (ffill):\")\n",
    "print(data.fillna(method='ffill'))\n",
    "\n",
    "# Backward fill - propagate next valid value backward\n",
    "print(\"\\nBackward fill (bfill):\")\n",
    "print(data.fillna(method='bfill'))\n",
    "\n",
    "# Compare all three\n",
    "print(\"\\n=== Comparison ===\")\n",
    "comparison = pd.DataFrame({\n",
    "    'Original': data,\n",
    "    'Forward Fill': data.fillna(method='ffill'),\n",
    "    'Backward Fill': data.fillna(method='bfill')\n",
    "})\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4106556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward and backward fill with DataFrames\n",
    "df_dates = pd.DataFrame({\n",
    "    'Date': pd.date_range('2024-01-01', periods=7, freq='D'),\n",
    "    'Sales': [100, np.nan, np.nan, 150, np.nan, 180, 200],\n",
    "    'Customers': [10, 12, np.nan, np.nan, 15, np.nan, 20]\n",
    "})\n",
    "\n",
    "print(\"Sales data with missing values:\")\n",
    "print(df_dates)\n",
    "\n",
    "# Forward fill\n",
    "print(\"\\nForward fill:\")\n",
    "print(df_dates.fillna(method='ffill'))\n",
    "\n",
    "# Backward fill\n",
    "print(\"\\nBackward fill:\")\n",
    "print(df_dates.fillna(method='bfill'))\n",
    "\n",
    "# Limit the number of consecutive fills\n",
    "print(\"\\nForward fill with limit=1 (only fill 1 consecutive NaN):\")\n",
    "print(df_dates.fillna(method='ffill', limit=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f8a624",
   "metadata": {},
   "source": [
    "### Interpolation for Numerical Data\n",
    "\n",
    "For numerical data, interpolation can provide more sophisticated filling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10343e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolation example\n",
    "df_interp = pd.DataFrame({\n",
    "    'Value': [1, np.nan, np.nan, 4, np.nan, 6, np.nan, np.nan, 9]\n",
    "})\n",
    "\n",
    "print(\"Original data:\")\n",
    "print(df_interp)\n",
    "\n",
    "# Linear interpolation (default)\n",
    "df_interp['Linear'] = df_interp['Value'].interpolate()\n",
    "print(\"\\nLinear interpolation:\")\n",
    "print(df_interp)\n",
    "\n",
    "# Polynomial interpolation\n",
    "df_interp['Polynomial'] = df_interp['Value'].interpolate(method='polynomial', order=2)\n",
    "print(\"\\nPolynomial interpolation:\")\n",
    "print(df_interp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b6def7",
   "metadata": {},
   "source": [
    "### Practical Tips for Handling Missing Data\n",
    "\n",
    "**When to use each approach:**\n",
    "\n",
    "1. **`dropna()`**: Use when:\n",
    "   - Missing data is minimal (< 5% of rows)\n",
    "   - Missing values are random (MCAR - Missing Completely At Random)\n",
    "   - You have enough data after dropping\n",
    "\n",
    "2. **`fillna(0)` or constant**: Use when:\n",
    "   - Zero/constant makes business sense (e.g., no sales = 0)\n",
    "   - You're creating indicator variables\n",
    "\n",
    "3. **`fillna(mean/median)`**: Use when:\n",
    "   - Data is numerical\n",
    "   - You want to preserve the distribution\n",
    "   - Mean for normal distributions, median for skewed data\n",
    "\n",
    "4. **Forward/Backward fill**: Use when:\n",
    "   - Data is time-series\n",
    "   - Values don't change rapidly\n",
    "   - Order matters\n",
    "\n",
    "5. **`interpolate()`**: Use when:\n",
    "   - Data is numerical and smooth\n",
    "   - You expect gradual changes\n",
    "   - Time-series with regular intervals\n",
    "\n",
    "**Example Decision Tree:**\n",
    "```\n",
    "Is the data time-series? \n",
    "├─ Yes → Use ffill/bfill or interpolate()\n",
    "└─ No → Is < 5% missing?\n",
    "    ├─ Yes → Use dropna()\n",
    "    └─ No → Use fillna(mean/median) or domain-specific imputation\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2667c281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical example: Complete workflow\n",
    "print(\"=== Real-world Missing Data Workflow ===\\n\")\n",
    "\n",
    "# 1. Create realistic messy data\n",
    "df_real = pd.DataFrame({\n",
    "    'Date': pd.date_range('2024-01-01', periods=10, freq='D'),\n",
    "    'Product': ['A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B'],\n",
    "    'Sales': [100, 150, np.nan, 180, 120, np.nan, np.nan, 200, 110, np.nan],\n",
    "    'Returns': [5, np.nan, 3, 4, np.nan, 2, 1, np.nan, 2, 3]\n",
    "})\n",
    "\n",
    "print(\"Original data:\")\n",
    "print(df_real)\n",
    "print(f\"\\nMissing data summary:\")\n",
    "print(df_real.isnull().sum())\n",
    "\n",
    "# 2. Analyze missing patterns\n",
    "missing_pct = (df_real.isnull().sum() / len(df_real)) * 100\n",
    "print(f\"\\nMissing percentage:\")\n",
    "print(missing_pct)\n",
    "\n",
    "# 3. Apply appropriate strategy\n",
    "df_clean = df_real.copy()\n",
    "\n",
    "# Sales: use forward fill (time series assumption)\n",
    "df_clean['Sales'] = df_clean['Sales'].fillna(method='ffill')\n",
    "\n",
    "# Returns: use median (sporadic missing, numerical)\n",
    "df_clean['Returns'] = df_clean['Returns'].fillna(df_clean['Returns'].median())\n",
    "\n",
    "print(\"\\nCleaned data:\")\n",
    "print(df_clean)\n",
    "\n",
    "# 4. Verify no missing values remain\n",
    "print(f\"\\nRemaining missing values:\")\n",
    "print(df_clean.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09f01ef",
   "metadata": {},
   "source": [
    "## Merging and Joining DataFrames\n",
    "\n",
    "Combining data from multiple sources is a fundamental task in data analysis. Pandas provides powerful tools for merging and joining DataFrames, similar to SQL database operations.\n",
    "\n",
    "**Documentation:** https://pandas.pydata.org/docs/user_guide/merging.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194c5434",
   "metadata": {},
   "source": [
    "### Understanding Different Types of Joins\n",
    "\n",
    "Before we dive into the code, it's important to understand the different types of joins and when to use each one:\n",
    "\n",
    "**1. One-to-One Join**: Each key appears only once in both DataFrames\n",
    "- Example: Merging employee personal info with employee contact info\n",
    "\n",
    "**2. Many-to-One Join**: Keys in one DataFrame appear multiple times, but only once in the other\n",
    "- Example: Merging employees with their department information\n",
    "\n",
    "**3. Many-to-Many Join**: Keys appear multiple times in both DataFrames\n",
    "- Example: Merging employees with skills (employees can have multiple skills, skills can belong to multiple employees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d59b5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: One-to-One Join\n",
    "# Each employee appears once in each DataFrame\n",
    "\n",
    "employees = pd.DataFrame({\n",
    "    'EmployeeID': [1, 2, 3, 4],\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David']\n",
    "})\n",
    "\n",
    "salaries = pd.DataFrame({\n",
    "    'EmployeeID': [1, 2, 3, 4],\n",
    "    'Salary': [70000, 80000, 75000, 90000]\n",
    "})\n",
    "\n",
    "print(\"One-to-One Join:\")\n",
    "print(\"Employees:\")\n",
    "print(employees)\n",
    "print(\"\\nSalaries:\")\n",
    "print(salaries)\n",
    "\n",
    "# Merge on EmployeeID\n",
    "result = pd.merge(employees, salaries, on='EmployeeID')\n",
    "print(\"\\nMerged Result (One-to-One):\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2384b5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Many-to-One Join\n",
    "# Multiple employees can belong to the same department\n",
    "\n",
    "employees = pd.DataFrame({\n",
    "    'EmployeeID': [1, 2, 3, 4, 5],\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "    'DepartmentID': [101, 102, 101, 103, 102]\n",
    "})\n",
    "\n",
    "departments = pd.DataFrame({\n",
    "    'DepartmentID': [101, 102, 103],\n",
    "    'Department': ['Sales', 'IT', 'HR'],\n",
    "    'Manager': ['John', 'Sarah', 'Mike']\n",
    "})\n",
    "\n",
    "print(\"Many-to-One Join:\")\n",
    "print(\"Employees:\")\n",
    "print(employees)\n",
    "print(\"\\nDepartments:\")\n",
    "print(departments)\n",
    "\n",
    "# Merge - each department info will be repeated for each employee\n",
    "result = pd.merge(employees, departments, on='DepartmentID')\n",
    "print(\"\\nMerged Result (Many-to-One):\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07f4286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Many-to-Many Join\n",
    "# Employees can have multiple skills, skills can belong to multiple employees\n",
    "\n",
    "employee_skills = pd.DataFrame({\n",
    "    'EmployeeID': [1, 1, 2, 2, 3, 3],\n",
    "    'Skill': ['Python', 'SQL', 'Python', 'Excel', 'SQL', 'Tableau']\n",
    "})\n",
    "\n",
    "skill_levels = pd.DataFrame({\n",
    "    'Skill': ['Python', 'Python', 'SQL', 'SQL', 'Excel', 'Tableau'],\n",
    "    'Level': ['Advanced', 'Beginner', 'Advanced', 'Intermediate', 'Advanced', 'Beginner']\n",
    "})\n",
    "\n",
    "print(\"Many-to-Many Join:\")\n",
    "print(\"Employee Skills:\")\n",
    "print(employee_skills)\n",
    "print(\"\\nSkill Levels:\")\n",
    "print(skill_levels)\n",
    "\n",
    "# This will create all possible combinations\n",
    "result = pd.merge(employee_skills, skill_levels, on='Skill')\n",
    "print(\"\\nMerged Result (Many-to-Many):\")\n",
    "print(result)\n",
    "print(\"\\nNote: Employee 1 with Python skill gets matched with both Python levels!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d24ab3",
   "metadata": {},
   "source": [
    "### Merging on Index: left_index and right_index\n",
    "\n",
    "Sometimes you want to merge DataFrames based on their index rather than a column. This is particularly useful when working with time series data or when the index contains meaningful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d617e428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrames with meaningful indices\n",
    "\n",
    "# Employee data indexed by employee ID\n",
    "employees = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
    "    'Age': [25, 30, 35, 28]\n",
    "}, index=[101, 102, 103, 104])\n",
    "employees.index.name = 'EmployeeID'\n",
    "\n",
    "# Salary data also indexed by employee ID\n",
    "salaries = pd.DataFrame({\n",
    "    'Salary': [70000, 80000, 75000, 90000],\n",
    "    'Bonus': [5000, 8000, 6000, 10000]\n",
    "}, index=[101, 102, 103, 104])\n",
    "salaries.index.name = 'EmployeeID'\n",
    "\n",
    "print(\"Employees (indexed by EmployeeID):\")\n",
    "print(employees)\n",
    "print(\"\\nSalaries (indexed by EmployeeID):\")\n",
    "print(salaries)\n",
    "\n",
    "# Merge on index using left_index and right_index\n",
    "result = pd.merge(employees, salaries, left_index=True, right_index=True)\n",
    "print(\"\\nMerged on Index:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fe0f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixing index and column merging\n",
    "\n",
    "# One DataFrame indexed, another with a column\n",
    "employees_indexed = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'Department': ['Sales', 'IT', 'Sales']\n",
    "}, index=[101, 102, 103])\n",
    "employees_indexed.index.name = 'EmployeeID'\n",
    "\n",
    "# This one has EmployeeID as a column\n",
    "performance = pd.DataFrame({\n",
    "    'EmployeeID': [101, 102, 103],\n",
    "    'Rating': [4.5, 4.8, 4.2],\n",
    "    'ReviewDate': ['2024-01-15', '2024-01-20', '2024-01-18']\n",
    "})\n",
    "\n",
    "print(\"Employees (indexed):\")\n",
    "print(employees_indexed)\n",
    "print(\"\\nPerformance (column-based):\")\n",
    "print(performance)\n",
    "\n",
    "# Merge: left_index=True, right_on='EmployeeID'\n",
    "result = pd.merge(employees_indexed, performance, \n",
    "                  left_index=True, right_on='EmployeeID')\n",
    "print(\"\\nMerged (index + column):\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c531adba",
   "metadata": {},
   "source": [
    "### Handling Column Name Conflicts with suffixes\n",
    "\n",
    "When merging DataFrames that have columns with the same name (other than the merge key), Pandas automatically adds suffixes to distinguish them. By default, it uses `_x` and `_y`, but you can customize this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d182d9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrames with conflicting column names\n",
    "\n",
    "# Employee data from HR system\n",
    "hr_data = pd.DataFrame({\n",
    "    'EmployeeID': [1, 2, 3],\n",
    "    'Name': ['Alice Smith', 'Bob Jones', 'Charlie Brown'],\n",
    "    'Department': ['Sales', 'IT', 'Sales'],\n",
    "    'Status': ['Active', 'Active', 'Active']\n",
    "})\n",
    "\n",
    "# Employee data from Payroll system\n",
    "payroll_data = pd.DataFrame({\n",
    "    'EmployeeID': [1, 2, 3],\n",
    "    'Name': ['A. Smith', 'R. Jones', 'C. Brown'],  # Different name format\n",
    "    'Department': ['Sales', 'IT', 'Marketing'],     # Might be different!\n",
    "    'Status': ['Paid', 'Paid', 'Pending']\n",
    "})\n",
    "\n",
    "print(\"HR Data:\")\n",
    "print(hr_data)\n",
    "print(\"\\nPayroll Data:\")\n",
    "print(payroll_data)\n",
    "\n",
    "# Merge with default suffixes (_x and _y)\n",
    "result_default = pd.merge(hr_data, payroll_data, on='EmployeeID')\n",
    "print(\"\\nMerged with default suffixes:\")\n",
    "print(result_default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c116e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using custom suffixes for clarity\n",
    "\n",
    "result_custom = pd.merge(hr_data, payroll_data, on='EmployeeID', \n",
    "                        suffixes=('_HR', '_Payroll'))\n",
    "print(\"Merged with custom suffixes:\")\n",
    "print(result_custom)\n",
    "\n",
    "# Now it's much clearer which data comes from which system!\n",
    "print(\"\\nCompare departments:\")\n",
    "print(result_custom[['EmployeeID', 'Name_HR', 'Department_HR', 'Department_Payroll']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055fa5ae",
   "metadata": {},
   "source": [
    "### Types of Joins: how parameter\n",
    "\n",
    "The `how` parameter in `pd.merge()` determines which keys to include in the result:\n",
    "\n",
    "- **`inner`** (default): Only keys that appear in both DataFrames\n",
    "- **`outer`**: All keys from both DataFrames\n",
    "- **`left`**: All keys from the left DataFrame\n",
    "- **`right`**: All keys from the right DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398f5a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data with some non-matching keys\n",
    "\n",
    "employees = pd.DataFrame({\n",
    "    'EmployeeID': [1, 2, 3, 4],\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David']\n",
    "})\n",
    "\n",
    "departments = pd.DataFrame({\n",
    "    'EmployeeID': [3, 4, 5, 6],\n",
    "    'Department': ['Sales', 'IT', 'HR', 'Marketing']\n",
    "})\n",
    "\n",
    "print(\"Employees:\")\n",
    "print(employees)\n",
    "print(\"\\nDepartments:\")\n",
    "print(departments)\n",
    "print(\"\\nNotice: IDs 1,2 only in employees; IDs 5,6 only in departments; IDs 3,4 in both\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d843f309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner join - only matching keys (3, 4)\n",
    "inner_result = pd.merge(employees, departments, on='EmployeeID', how='inner')\n",
    "print(\"Inner Join (only IDs 3, 4):\")\n",
    "print(inner_result)\n",
    "\n",
    "# Outer join - all keys from both\n",
    "outer_result = pd.merge(employees, departments, on='EmployeeID', how='outer')\n",
    "print(\"\\nOuter Join (all IDs):\")\n",
    "print(outer_result)\n",
    "\n",
    "# Left join - all keys from left (employees)\n",
    "left_result = pd.merge(employees, departments, on='EmployeeID', how='left')\n",
    "print(\"\\nLeft Join (IDs 1, 2, 3, 4):\")\n",
    "print(left_result)\n",
    "\n",
    "# Right join - all keys from right (departments)\n",
    "right_result = pd.merge(employees, departments, on='EmployeeID', how='right')\n",
    "print(\"\\nRight Join (IDs 3, 4, 5, 6):\")\n",
    "print(right_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcadbd0",
   "metadata": {},
   "source": [
    "### Practical Example: Combining Employee Data\n",
    "\n",
    "Let's put it all together with a realistic example combining multiple data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeafe48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario: Combine employee data from multiple systems\n",
    "\n",
    "# 1. Basic employee info\n",
    "employees = pd.DataFrame({\n",
    "    'EmployeeID': [101, 102, 103, 104, 105],\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "    'DepartmentID': [1, 2, 1, 3, 2]\n",
    "})\n",
    "\n",
    "# 2. Department information\n",
    "departments = pd.DataFrame({\n",
    "    'DepartmentID': [1, 2, 3, 4],\n",
    "    'Department': ['Sales', 'IT', 'HR', 'Marketing'],\n",
    "    'Location': ['New York', 'San Francisco', 'Chicago', 'Boston']\n",
    "})\n",
    "\n",
    "# 3. Salary information (not all employees have salary data yet)\n",
    "salaries = pd.DataFrame({\n",
    "    'EmployeeID': [101, 102, 103, 105],\n",
    "    'Salary': [70000, 85000, 72000, 90000]\n",
    "})\n",
    "\n",
    "print(\"Step 1: Merge employees with departments (many-to-one)\")\n",
    "df = pd.merge(employees, departments, on='DepartmentID', how='left')\n",
    "print(df)\n",
    "\n",
    "print(\"\\nStep 2: Merge with salaries (one-to-one, left join to keep all employees)\")\n",
    "final = pd.merge(df, salaries, on='EmployeeID', how='left')\n",
    "print(final)\n",
    "\n",
    "print(\"\\nFinal Result Summary:\")\n",
    "print(f\"Total employees: {len(final)}\")\n",
    "print(f\"Employees with salary data: {final['Salary'].notna().sum()}\")\n",
    "print(f\"Employees missing salary: {final['Salary'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444f537a",
   "metadata": {},
   "source": [
    "### Key Takeaways for Merging\n",
    "\n",
    "**When to use each join type:**\n",
    "\n",
    "1. **Inner join** (`how='inner'`):\n",
    "   - When you only want records that have matches in both DataFrames\n",
    "   - Safest option when you want complete data\n",
    "\n",
    "2. **Outer join** (`how='outer'`):\n",
    "   - When you want to keep all records from both DataFrames\n",
    "   - Useful for finding gaps in data\n",
    "\n",
    "3. **Left join** (`how='left'`):\n",
    "   - When the left DataFrame is your main data\n",
    "   - Want to enrich it with additional info from right\n",
    "   - Most common in practice\n",
    "\n",
    "4. **Right join** (`how='right'`):\n",
    "   - Rarely used (can use left join by swapping DataFrames)\n",
    "   - Included for completeness\n",
    "\n",
    "**Best Practices:**\n",
    "- Always check the shape before and after merging\n",
    "- Use `indicator=True` to track merge status\n",
    "- Be explicit with column names using `left_on`/`right_on`\n",
    "- Use meaningful suffixes when columns conflict\n",
    "- Consider using `validate` parameter to catch unexpected duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e326e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced: Using indicator to track merge status\n",
    "\n",
    "result = pd.merge(employees, salaries, on='EmployeeID', how='outer', indicator=True)\n",
    "print(\"Merge with indicator:\")\n",
    "print(result)\n",
    "\n",
    "print(\"\\nMerge statistics:\")\n",
    "print(result['_merge'].value_counts())\n",
    "\n",
    "# Find employees without salary data\n",
    "print(\"\\nEmployees missing salary:\")\n",
    "print(result[result['_merge'] == 'left_only'][['EmployeeID', 'Name']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189c7f63",
   "metadata": {},
   "source": [
    "## Adding and Modifying Data\n",
    "\n",
    "Before working with data, you often need to add or modify columns and rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc67aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'Age': [25, 30, 35],\n",
    "    'Salary': [50000, 60000, 55000]\n",
    "})\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Add a new column\n",
    "df['City'] = ['New York', 'London', 'Paris']\n",
    "print(\"\\nAfter adding 'City' column:\")\n",
    "print(df)\n",
    "\n",
    "# Add calculated column\n",
    "df['Salary_k'] = df['Salary'] / 1000\n",
    "print(\"\\nAfter adding calculated column:\")\n",
    "print(df)\n",
    "\n",
    "# Modify existing column\n",
    "df['Age'] = df['Age'] + 1\n",
    "print(\"\\nAfter incrementing Age:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba854a4",
   "metadata": {},
   "source": [
    "## Sorting Data\n",
    "\n",
    "Sorting is essential for organizing and analyzing data.\n",
    "\n",
    "**Documentation:** https://pandas.pydata.org/docs/user_guide/basics.html#sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35b61d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Name': ['Charlie', 'Alice', 'Eve', 'Bob', 'David'],\n",
    "    'Age': [35, 25, 32, 30, 28],\n",
    "    'Salary': [55000, 50000, 58000, 60000, 65000]\n",
    "})\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Sort by single column\n",
    "df_sorted = df.sort_values('Age')\n",
    "print(\"\\nSorted by Age:\")\n",
    "print(df_sorted)\n",
    "\n",
    "# Sort in descending order\n",
    "df_sorted_desc = df.sort_values('Salary', ascending=False)\n",
    "print(\"\\nSorted by Salary (descending):\")\n",
    "print(df_sorted_desc)\n",
    "\n",
    "# Sort by multiple columns\n",
    "df_sorted_multi = df.sort_values(['Age', 'Salary'], ascending=[True, False])\n",
    "print(\"\\nSorted by Age (asc) then Salary (desc):\")\n",
    "print(df_sorted_multi)\n",
    "\n",
    "# Sort by index\n",
    "df_sorted_index = df.sort_index()\n",
    "print(\"\\nSorted by index:\")\n",
    "print(df_sorted_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1824bd6",
   "metadata": {},
   "source": [
    "## Handling Duplicate Data\n",
    "\n",
    "Duplicate rows are common in real datasets. Pandas provides methods to detect and remove them:\n",
    "\n",
    "**Documentation:** https://pandas.pydata.org/docs/user_guide/duplicates.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05580c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with duplicates\n",
    "df = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Alice', 'Charlie', 'Bob', 'David'],\n",
    "    'Age': [25, 30, 25, 35, 30, 28],\n",
    "    'City': ['New York', 'London', 'New York', 'Paris', 'London', 'Tokyo']\n",
    "})\n",
    "\n",
    "print(\"DataFrame with duplicates:\")\n",
    "print(df)\n",
    "\n",
    "# Check for duplicates\n",
    "print(\"\\nAre there duplicates?:\", df.duplicated().any())\n",
    "print(\"\\nDuplicate rows (boolean mask):\")\n",
    "print(df.duplicated())\n",
    "\n",
    "# Show duplicate rows\n",
    "print(\"\\nDuplicate rows:\")\n",
    "print(df[df.duplicated()])\n",
    "\n",
    "# Count duplicates\n",
    "print(f\"\\nNumber of duplicate rows: {df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8448f255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates (keep first occurrence)\n",
    "df_no_duplicates = df.drop_duplicates()\n",
    "print(\"After removing duplicates (keep first):\")\n",
    "print(df_no_duplicates)\n",
    "\n",
    "# Remove duplicates (keep last occurrence)\n",
    "df_keep_last = df.drop_duplicates(keep='last')\n",
    "print(\"\\nAfter removing duplicates (keep last):\")\n",
    "print(df_keep_last)\n",
    "\n",
    "# Remove duplicates based on specific columns\n",
    "df_partial = df.drop_duplicates(subset=['Name'])\n",
    "print(\"\\nRemove duplicates based on 'Name' only:\")\n",
    "print(df_partial)\n",
    "\n",
    "# Keep none (remove all duplicates including originals)\n",
    "df_keep_none = df.drop_duplicates(keep=False)\n",
    "print(\"\\nRemove all duplicates (including originals):\")\n",
    "print(df_keep_none)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdab807",
   "metadata": {},
   "source": [
    "## Value Counts and Frequency Analysis\n",
    "\n",
    "Understanding the distribution of values in your data is crucial for exploratory data analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf93bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "df = pd.DataFrame({\n",
    "    'Product': ['Laptop', 'Phone', 'Tablet', 'Laptop', 'Phone', 'Phone', 'Tablet', 'Laptop'],\n",
    "    'Region': ['North', 'South', 'North', 'East', 'North', 'South', 'West', 'North'],\n",
    "    'Status': ['Sold', 'Sold', 'Pending', 'Sold', 'Sold', 'Pending', 'Sold', 'Sold']\n",
    "})\n",
    "\n",
    "print(\"Sales data:\")\n",
    "print(df)\n",
    "\n",
    "# Count unique values in a column\n",
    "print(\"\\nProduct value counts:\")\n",
    "print(df['Product'].value_counts())\n",
    "\n",
    "# With percentages\n",
    "print(\"\\nProduct value counts (percentages):\")\n",
    "print(df['Product'].value_counts(normalize=True))\n",
    "\n",
    "# Count unique values\n",
    "print(f\"\\nNumber of unique products: {df['Product'].nunique()}\")\n",
    "print(f\"Unique products: {df['Product'].unique()}\")\n",
    "\n",
    "# Crosstab for frequency analysis\n",
    "print(\"\\nCrosstab - Product vs Region:\")\n",
    "print(pd.crosstab(df['Product'], df['Region']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7d1807",
   "metadata": {},
   "source": [
    "## Grouping and Aggregation\n",
    "\n",
    "GroupBy allows you to split data into groups and apply functions to each group:\n",
    "\n",
    "**Documentation:** https://pandas.pydata.org/docs/user_guide/groupby.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95710178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Department': ['Sales', 'Sales', 'IT', 'IT', 'HR', 'HR', 'Sales'],\n",
    "    'Employee': ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Frank', 'Grace'],\n",
    "    'Age': [25, 30, 35, 28, 32, 45, 27],\n",
    "    'Salary': [50000, 60000, 55000, 65000, 58000, 52000, 54000]\n",
    "})\n",
    "\n",
    "print(\"Employee DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Group by single column\n",
    "grouped = df.groupby('Department')\n",
    "\n",
    "# Calculate mean per group\n",
    "print(\"\\nMean salary by department:\")\n",
    "print(grouped['Salary'].mean())\n",
    "\n",
    "# Multiple aggregations\n",
    "print(\"\\nMultiple statistics by department:\")\n",
    "print(grouped['Salary'].agg(['mean', 'min', 'max', 'count']))\n",
    "\n",
    "# Group by and aggregate different columns differently\n",
    "print(\"\\nDifferent aggregations per column:\")\n",
    "print(grouped.agg({\n",
    "    'Age': ['mean', 'min', 'max'],\n",
    "    'Salary': ['mean', 'sum']\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059d9ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple grouping columns\n",
    "df['Experience'] = ['Junior', 'Senior', 'Senior', 'Junior', 'Senior', 'Senior', 'Junior']\n",
    "\n",
    "print(\"DataFrame with Experience:\")\n",
    "print(df)\n",
    "\n",
    "# Group by multiple columns\n",
    "grouped_multi = df.groupby(['Department', 'Experience'])\n",
    "print(\"\\nMean salary by Department and Experience:\")\n",
    "print(grouped_multi['Salary'].mean())\n",
    "\n",
    "# Reset index to make it a regular DataFrame\n",
    "print(\"\\nWith reset index:\")\n",
    "print(grouped_multi['Salary'].mean().reset_index())\n",
    "\n",
    "# Size of each group\n",
    "print(\"\\nCount of employees per group:\")\n",
    "print(grouped_multi.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c134e154",
   "metadata": {},
   "source": [
    "## Apply and Lambda Functions\n",
    "\n",
    "The `apply()` function allows you to apply custom transformations to your data:\n",
    "\n",
    "**Documentation:** https://pandas.pydata.org/docs/user_guide/basics.html#function-application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3864c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
    "    'Age': [25, 30, 35, 28],\n",
    "    'Salary': [50000, 60000, 55000, 65000]\n",
    "})\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Apply lambda to a single column\n",
    "df['Age_in_10_years'] = df['Age'].apply(lambda x: x + 10)\n",
    "print(\"\\nWith Age_in_10_years:\")\n",
    "print(df)\n",
    "\n",
    "# Apply with conditional logic\n",
    "df['Age_Group'] = df['Age'].apply(lambda x: 'Young' if x < 30 else 'Senior')\n",
    "print(\"\\nWith Age_Group:\")\n",
    "print(df)\n",
    "\n",
    "# Apply to entire DataFrame (axis=1 for rows)\n",
    "df['Total_Score'] = df.apply(lambda row: (row['Age'] * 0.3) + (row['Salary'] / 1000), axis=1)\n",
    "print(\"\\nWith calculated Total_Score:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1457a30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom function\n",
    "def categorize_salary(salary):\n",
    "    if salary < 55000:\n",
    "        return 'Low'\n",
    "    elif salary < 62000:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'High'\n",
    "\n",
    "df['Salary_Category'] = df['Salary'].apply(categorize_salary)\n",
    "print(\"With Salary_Category:\")\n",
    "print(df)\n",
    "\n",
    "# Apply with multiple columns\n",
    "def full_description(row):\n",
    "    return f\"{row['Name']} is {row['Age']} years old and earns ${row['Salary']:,}\"\n",
    "\n",
    "df['Description'] = df.apply(full_description, axis=1)\n",
    "print(\"\\nWith Description:\")\n",
    "print(df[['Name', 'Description']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c2ff04",
   "metadata": {},
   "source": [
    "## Pivot Tables and Reshaping\n",
    "\n",
    "Transform data between wide and long formats:\n",
    "\n",
    "**Documentation:** https://pandas.pydata.org/docs/user_guide/reshaping.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b106ea46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample sales data\n",
    "sales = pd.DataFrame({\n",
    "    'Date': ['2024-01', '2024-01', '2024-02', '2024-02', '2024-03', '2024-03'],\n",
    "    'Product': ['A', 'B', 'A', 'B', 'A', 'B'],\n",
    "    'Sales': [100, 150, 120, 180, 110, 160],\n",
    "    'Region': ['East', 'East', 'West', 'West', 'East', 'East']\n",
    "})\n",
    "\n",
    "print(\"Sales data:\")\n",
    "print(sales)\n",
    "\n",
    "# Create pivot table\n",
    "pivot = sales.pivot_table(values='Sales', \n",
    "                          index='Date', \n",
    "                          columns='Product', \n",
    "                          aggfunc='sum')\n",
    "print(\"\\nPivot table (Sales by Date and Product):\")\n",
    "print(pivot)\n",
    "\n",
    "# Pivot with multiple aggregations\n",
    "pivot_multi = sales.pivot_table(values='Sales', \n",
    "                                index='Date', \n",
    "                                columns='Product', \n",
    "                                aggfunc=['sum', 'mean'])\n",
    "print(\"\\nPivot with multiple aggregations:\")\n",
    "print(pivot_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ac9589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt (wide to long format)\n",
    "df_wide = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'Math': [90, 85, 95],\n",
    "    'English': [88, 92, 89]\n",
    "})\n",
    "\n",
    "print(\"Wide format:\")\n",
    "print(df_wide)\n",
    "\n",
    "df_long = pd.melt(df_wide, \n",
    "                  id_vars=['Name'], \n",
    "                  value_vars=['Math', 'English'],\n",
    "                  var_name='Subject', \n",
    "                  value_name='Score')\n",
    "print(\"\\nLong format (melted):\")\n",
    "print(df_long)\n",
    "\n",
    "# Pivot (long to wide format)\n",
    "df_wide_again = df_long.pivot(index='Name', columns='Subject', values='Score')\n",
    "print(\"\\nBack to wide format:\")\n",
    "print(df_wide_again)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb7cd73",
   "metadata": {},
   "source": [
    "## Reading and Writing Data\n",
    "\n",
    "Pandas can read/write data from/to various formats:\n",
    "\n",
    "**Documentation:** https://pandas.pydata.org/docs/user_guide/io.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1de02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
    "    'Age': [25, 30, 35, 28],\n",
    "    'City': ['New York', 'London', 'Paris', 'Tokyo'],\n",
    "    'Salary': [50000, 60000, 55000, 65000]\n",
    "})\n",
    "\n",
    "# Write to CSV\n",
    "df.to_csv('employees.csv', index=False)\n",
    "print(\"Written to employees.csv\")\n",
    "\n",
    "# Read from CSV\n",
    "df_from_csv = pd.read_csv('employees.csv')\n",
    "print(\"\\nRead from CSV:\")\n",
    "print(df_from_csv)\n",
    "\n",
    "# Write to JSON\n",
    "df.to_json('employees.json', orient='records', indent=2)\n",
    "print(\"\\nWritten to employees.json\")\n",
    "\n",
    "# Read from JSON\n",
    "df_from_json = pd.read_json('employees.json')\n",
    "print(\"\\nRead from JSON:\")\n",
    "print(df_from_json)\n",
    "\n",
    "# Common read_csv parameters:\n",
    "# pd.read_csv('file.csv', sep=',', header=0, names=['col1', 'col2'], \n",
    "#             index_col=0, usecols=['col1', 'col2'], nrows=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e34ff5",
   "metadata": {},
   "source": [
    "## String Operations\n",
    "\n",
    "Pandas provides powerful string manipulation methods:\n",
    "\n",
    "**Documentation:** https://pandas.pydata.org/docs/user_guide/text.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f219ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with string data\n",
    "df = pd.DataFrame({\n",
    "    'Name': ['alice smith', 'BOB JONES', 'Charlie Brown', 'david LEE'],\n",
    "    'Email': ['alice@example.com', 'BOB@EXAMPLE.COM', 'charlie@test.org', 'david@sample.net']\n",
    "})\n",
    "\n",
    "print(\"Original:\")\n",
    "print(df)\n",
    "\n",
    "# Convert to lowercase\n",
    "df['Name_lower'] = df['Name'].str.lower()\n",
    "\n",
    "# Convert to uppercase\n",
    "df['Name_upper'] = df['Name'].str.upper()\n",
    "\n",
    "# Title case\n",
    "df['Name_title'] = df['Name'].str.title()\n",
    "\n",
    "print(\"\\nWith case conversions:\")\n",
    "print(df[['Name', 'Name_lower', 'Name_upper', 'Name_title']])\n",
    "\n",
    "# Extract domain from email\n",
    "df['Domain'] = df['Email'].str.split('@').str[1]\n",
    "\n",
    "# Check if contains substring\n",
    "df['Has_example'] = df['Email'].str.contains('example')\n",
    "\n",
    "print(\"\\nWith string operations:\")\n",
    "print(df[['Email', 'Domain', 'Has_example']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba61127f",
   "metadata": {},
   "source": [
    "## Date and Time Operations\n",
    "\n",
    "Pandas has excellent support for working with dates and times:\n",
    "\n",
    "**Documentation:** https://pandas.pydata.org/docs/user_guide/timeseries.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e5fba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with date strings\n",
    "df = pd.DataFrame({\n",
    "    'Date': ['2024-01-15', '2024-02-20', '2024-03-25', '2024-04-30'],\n",
    "    'Sales': [1000, 1500, 1200, 1800]\n",
    "})\n",
    "\n",
    "print(\"Original:\")\n",
    "print(df)\n",
    "print(\"\\nDate dtype:\", df['Date'].dtype)\n",
    "\n",
    "# Convert to datetime\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "print(\"\\nAfter conversion:\")\n",
    "print(df)\n",
    "print(\"Date dtype:\", df['Date'].dtype)\n",
    "\n",
    "# Extract date components\n",
    "df['Year'] = df['Date'].dt.year\n",
    "df['Month'] = df['Date'].dt.month\n",
    "df['Day'] = df['Date'].dt.day\n",
    "df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
    "df['DayName'] = df['Date'].dt.day_name()\n",
    "\n",
    "print(\"\\nWith extracted components:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c7fbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date range\n",
    "date_range = pd.date_range(start='2024-01-01', end='2024-01-10', freq='D')\n",
    "print(\"Date range:\")\n",
    "print(date_range)\n",
    "\n",
    "# Create DataFrame with date range\n",
    "df_dates = pd.DataFrame({\n",
    "    'Date': pd.date_range('2024-01-01', periods=10, freq='D'),\n",
    "    'Value': np.random.randint(100, 200, 10)\n",
    "})\n",
    "\n",
    "print(\"\\nDataFrame with date range:\")\n",
    "print(df_dates)\n",
    "\n",
    "# Set date as index\n",
    "df_dates.set_index('Date', inplace=True)\n",
    "print(\"\\nWith date as index:\")\n",
    "print(df_dates)\n",
    "\n",
    "# Select by date\n",
    "print(\"\\nData for 2024-01-05:\")\n",
    "print(df_dates.loc['2024-01-05'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107be4f2",
   "metadata": {},
   "source": [
    "## Basic Visualization\n",
    "\n",
    "Pandas has built-in plotting capabilities using Matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2c1fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create sample data\n",
    "df = pd.DataFrame({\n",
    "    'Month': ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun'],\n",
    "    'Sales': [1000, 1500, 1200, 1800, 2100, 1900],\n",
    "    'Expenses': [800, 900, 850, 950, 1100, 1000]\n",
    "})\n",
    "\n",
    "# Line plot\n",
    "df.plot(x='Month', y=['Sales', 'Expenses'], kind='line', figsize=(10, 5))\n",
    "plt.title('Sales vs Expenses')\n",
    "plt.ylabel('Amount ($)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Bar plot\n",
    "df.plot(x='Month', y='Sales', kind='bar', figsize=(10, 5), color='skyblue')\n",
    "plt.title('Monthly Sales')\n",
    "plt.ylabel('Sales ($)')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dcc8cf",
   "metadata": {},
   "source": [
    "## Practical Exercises\n",
    "\n",
    "These exercises are designed to help you practice the concepts covered in this notebook. They progress from basic to advanced topics.\n",
    "\n",
    "**How to use these exercises:**\n",
    "1. Read the problem description carefully\n",
    "2. Try to solve it on your own first\n",
    "3. Use the sample data provided\n",
    "4. Check your understanding by running the code\n",
    "5. Don't be afraid to look back at the examples above\n",
    "\n",
    "**Tip:** Create a copy of this notebook and add your solutions in new cells below each exercise!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898d5dc3",
   "metadata": {},
   "source": [
    "#### Exercise 1: Create and Explore a DataFrame\n",
    "\n",
    "**Task:** Create a DataFrame with information about students and calculate basic statistics.\n",
    "\n",
    "**Requirements:**\n",
    "1. Create a DataFrame with at least 5 students\n",
    "2. Include columns: Name, Age, Math_Grade, English_Grade, Science_Grade\n",
    "3. Calculate the average grade for each student (add a new column called 'Average')\n",
    "4. Find the student with the highest average grade\n",
    "5. Calculate the mean, max, and min for each subject across all students\n",
    "\n",
    "**Sample data to get you started:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7e374e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your solution here\n",
    "# Sample data structure:\n",
    "students_data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
    "    'Age': [18, 19, 18, 20, 19],\n",
    "    'Math_Grade': [85, 92, 78, 88, 95],\n",
    "    'English_Grade': [90, 85, 92, 87, 89],\n",
    "    'Science_Grade': [88, 90, 85, 91, 93]\n",
    "}\n",
    "\n",
    "# TODO: Create DataFrame\n",
    "# TODO: Add 'Average' column\n",
    "# TODO: Find student with highest average\n",
    "# TODO: Calculate statistics for each subject"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43520cd",
   "metadata": {},
   "source": [
    "#### Exercise 2: Selection with loc and iloc\n",
    "\n",
    "**Task:** Practice different selection methods using the DataFrame below.\n",
    "\n",
    "**Requirements:**\n",
    "1. Select the row for 'Bob' using `.loc[]`\n",
    "2. Select the first 3 rows using `.iloc[]`\n",
    "3. Select Age and Salary columns for employees older than 30\n",
    "4. Select Name and City for employees in positions 2 and 4 using `.iloc[]`\n",
    "5. Explain the difference between your `.loc[]` and `.iloc[]` selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a32f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Sample data\n",
    "employees = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve', 'Frank'],\n",
    "    'Age': [25, 30, 35, 28, 32, 40],\n",
    "    'City': ['NYC', 'LA', 'Chicago', 'Houston', 'Phoenix', 'Miami'],\n",
    "    'Salary': [50000, 60000, 55000, 65000, 58000, 70000]\n",
    "}, index=['E001', 'E002', 'E003', 'E004', 'E005', 'E006'])\n",
    "\n",
    "print(\"Employee Data:\")\n",
    "print(employees)\n",
    "\n",
    "# TODO: Your selections here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f3eb4f",
   "metadata": {},
   "source": [
    "#### Exercise 3: Filtering with Boolean Indexing\n",
    "\n",
    "**Task:** Filter data based on multiple conditions.\n",
    "\n",
    "**Requirements:**\n",
    "1. Find all products with price > 500 AND stock < 50\n",
    "2. Find products that are either 'Electronics' OR have rating >= 4.5\n",
    "3. Find products whose name starts with 'L'\n",
    "4. Count how many products meet each condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd30264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Sample data\n",
    "products = pd.DataFrame({\n",
    "    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Laptop Pro', 'Headphones'],\n",
    "    'Category': ['Electronics', 'Accessories', 'Accessories', 'Electronics', 'Electronics', 'Accessories'],\n",
    "    'Price': [999, 25, 75, 350, 1500, 120],\n",
    "    'Stock': [30, 200, 150, 45, 15, 80],\n",
    "    'Rating': [4.5, 4.2, 4.7, 4.6, 4.8, 4.3]\n",
    "})\n",
    "\n",
    "print(\"Product Catalog:\")\n",
    "print(products)\n",
    "\n",
    "# TODO: Apply filters here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5dceda",
   "metadata": {},
   "source": [
    "#### Exercise 4: Sorting Data\n",
    "\n",
    "**Task:** Sort data by different criteria.\n",
    "\n",
    "**Requirements:**\n",
    "1. Sort by Salary in descending order\n",
    "2. Sort by Department (ascending) and then by Salary (descending)\n",
    "3. Find the top 3 highest-paid employees\n",
    "4. Sort by Age and reset the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e2ac2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Sample data\n",
    "company = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve', 'Frank', 'Grace'],\n",
    "    'Department': ['IT', 'Sales', 'IT', 'Sales', 'HR', 'IT', 'HR'],\n",
    "    'Age': [25, 30, 35, 28, 32, 40, 27],\n",
    "    'Salary': [70000, 65000, 80000, 68000, 62000, 85000, 60000]\n",
    "})\n",
    "\n",
    "print(\"Company Data:\")\n",
    "print(company)\n",
    "\n",
    "# TODO: Perform sorting operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bc62d9",
   "metadata": {},
   "source": [
    "#### Exercise 5: Handling Missing Data\n",
    "\n",
    "**Task:** Clean a messy dataset with missing values.\n",
    "\n",
    "**Requirements:**\n",
    "1. Identify which columns have missing values and how many\n",
    "2. Fill missing ages with the median age\n",
    "3. Fill missing cities with 'Unknown'\n",
    "4. Drop rows where Salary is missing\n",
    "5. Calculate the percentage of data that was missing before cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf35041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5: Sample data with missing values\n",
    "messy_data = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve', 'Frank'],\n",
    "    'Age': [25, np.nan, 35, 28, np.nan, 40],\n",
    "    'City': ['NYC', 'LA', None, 'Houston', None, 'Miami'],\n",
    "    'Salary': [50000, 60000, np.nan, 65000, 58000, 70000],\n",
    "    'Department': ['IT', 'Sales', 'IT', None, 'HR', 'IT']\n",
    "})\n",
    "\n",
    "print(\"Messy Data:\")\n",
    "print(messy_data)\n",
    "print(\"\\nMissing values:\")\n",
    "print(messy_data.isnull().sum())\n",
    "\n",
    "# TODO: Clean the data following the requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c77190",
   "metadata": {},
   "source": [
    "#### Exercise 6: Removing Duplicates\n",
    "\n",
    "**Task:** Identify and handle duplicate records in a transaction log.\n",
    "\n",
    "**Requirements:**\n",
    "1. Find all duplicate transactions (complete duplicates)\n",
    "2. Find transactions that have the same Customer and Product (partial duplicates)\n",
    "3. Keep only the first occurrence of each complete duplicate\n",
    "4. For partial duplicates based on Customer+Product, keep the one with the highest Amount\n",
    "5. Report how many duplicates were found and removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8877b696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 6: Sample transaction data\n",
    "transactions = pd.DataFrame({\n",
    "    'TransactionID': [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "    'Customer': ['Alice', 'Bob', 'Alice', 'Charlie', 'Bob', 'Alice', 'Diana', 'Charlie'],\n",
    "    'Product': ['Laptop', 'Mouse', 'Laptop', 'Keyboard', 'Mouse', 'Laptop', 'Monitor', 'Keyboard'],\n",
    "    'Amount': [999, 25, 999, 75, 25, 1200, 350, 75],\n",
    "    'Date': ['2024-01-15', '2024-01-16', '2024-01-15', '2024-01-17', \n",
    "             '2024-01-16', '2024-01-18', '2024-01-19', '2024-01-17']\n",
    "})\n",
    "\n",
    "print(\"Transaction Log:\")\n",
    "print(transactions)\n",
    "\n",
    "# TODO: Handle duplicates following the requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847382ca",
   "metadata": {},
   "source": [
    "#### Exercise 7: Value Counts and Frequency Analysis\n",
    "\n",
    "**Task:** Analyze sales data to understand product and regional distribution.\n",
    "\n",
    "**Requirements:**\n",
    "1. Count how many times each product was sold\n",
    "2. Show the percentage distribution of sales by region\n",
    "3. Create a crosstab showing Product vs Status\n",
    "4. Find the most popular product in each region\n",
    "5. Identify products that were never marked as 'Returned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d604f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 7: Sample sales data\n",
    "sales_log = pd.DataFrame({\n",
    "    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Laptop', 'Mouse', 'Monitor', \n",
    "                'Laptop', 'Keyboard', 'Mouse', 'Monitor', 'Laptop', 'Mouse'],\n",
    "    'Region': ['North', 'South', 'North', 'East', 'North', 'West', \n",
    "               'South', 'East', 'South', 'North', 'West', 'East'],\n",
    "    'Status': ['Sold', 'Sold', 'Sold', 'Sold', 'Returned', 'Sold', \n",
    "               'Sold', 'Sold', 'Sold', 'Returned', 'Sold', 'Sold'],\n",
    "    'Quantity': [1, 2, 1, 1, 1, 1, 1, 3, 1, 1,\n",
    "                 2, 1, 3, 2, 2, 1, 4, 1, 2, 1]\n",
    "})\n",
    "\n",
    "print(\"Sales Log:\")\n",
    "print(sales_log)\n",
    "\n",
    "# TODO: Perform frequency analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8e8a0f",
   "metadata": {},
   "source": [
    "#### Exercise 8: String Operations\n",
    "\n",
    "**Task:** Clean and standardize customer email data.\n",
    "\n",
    "**Requirements:**\n",
    "1. Convert all names to Title Case (first letter uppercase)\n",
    "2. Convert all emails to lowercase\n",
    "3. Extract the domain from each email (part after @)\n",
    "4. Create a boolean column 'Has_Gmail' that is True if email contains 'gmail'\n",
    "5. Split the Name into 'First_Name' and 'Last_Name' columns\n",
    "6. Count how many customers use each email domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446c95af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 8: Sample customer data\n",
    "customers = pd.DataFrame({\n",
    "    'Name': ['alice SMITH', 'bob jones', 'CHARLIE BROWN', 'diana PRINCE'],\n",
    "    'Email': ['Alice.Smith@GMAIL.com', 'bob@yahoo.com', 'charlie@GMAIL.COM', 'diana@outlook.com'],\n",
    "    'Phone': ['555-0101', '555-0102', '555-0103', '555-0104']\n",
    "})\n",
    "\n",
    "print(\"Customer Data:\")\n",
    "print(customers)\n",
    "\n",
    "# TODO: Clean and extract information from strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e67ba0",
   "metadata": {},
   "source": [
    "#### Exercise 9: Date and Time Operations\n",
    "\n",
    "**Task:** Analyze sales trends over time.\n",
    "\n",
    "**Requirements:**\n",
    "1. Convert the Date column to datetime format\n",
    "2. Extract Year, Month, Day, and Day of Week\n",
    "3. Add a column for the Quarter (Q1, Q2, Q3, Q4)\n",
    "4. Filter sales from January 2024\n",
    "5. Calculate total sales by month\n",
    "6. Find the day of the week with the highest average sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444c5ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 9: Sample time series data\n",
    "sales_dates = pd.DataFrame({\n",
    "    'Date': ['2024-01-15', '2024-01-22', '2024-02-10', '2024-02-18', \n",
    "             '2024-03-05', '2024-03-20', '2024-04-12', '2024-04-25'],\n",
    "    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Laptop', 'Mouse', 'Keyboard', 'Laptop'],\n",
    "    'Sales_Amount': [999, 25, 75, 350, 1200, 30, 80, 1100]\n",
    "})\n",
    "\n",
    "print(\"Sales with Dates:\")\n",
    "print(sales_dates)\n",
    "\n",
    "# TODO: Perform date operations and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde34b9f",
   "metadata": {},
   "source": [
    "#### Exercise 10: GroupBy and Aggregation\n",
    "\n",
    "**Task:** Analyze employee data by department.\n",
    "\n",
    "**Requirements:**\n",
    "1. Calculate average, min, and max salary by Department\n",
    "2. Count the number of employees in each Department\n",
    "3. Find the department with the highest average salary\n",
    "4. Group by Department and Experience level, calculate mean salary\n",
    "5. Create a summary showing total salary cost per department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021d4528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 10: Sample employee data\n",
    "employees_full = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve', 'Frank', 'Grace', 'Henry'],\n",
    "    'Department': ['IT', 'Sales', 'IT', 'Sales', 'HR', 'IT', 'HR', 'Sales'],\n",
    "    'Experience': ['Junior', 'Senior', 'Senior', 'Junior', 'Senior', 'Senior', 'Junior', 'Senior'],\n",
    "    'Age': [25, 30, 35, 28, 32, 40, 27, 38],\n",
    "    'Salary': [60000, 75000, 90000, 65000, 70000, 95000, 58000, 80000]\n",
    "})\n",
    "\n",
    "print(\"Employee Data:\")\n",
    "print(employees_full)\n",
    "\n",
    "# TODO: Perform group by operations and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d2635f",
   "metadata": {},
   "source": [
    "#### Exercise 11: Apply and Lambda Functions\n",
    "\n",
    "**Task:** Create custom calculations and categories.\n",
    "\n",
    "**Requirements:**\n",
    "1. Create a 'Salary_Category' column: Low (<65k), Medium (65-85k), High (>85k)\n",
    "2. Create an 'Age_Group' column: Young (<30), Mid (30-35), Senior (>35)\n",
    "3. Calculate a 'Performance_Score' using: (Salary/1000) * 0.7 + Age * 0.3\n",
    "4. Create a 'Full_Description' column with format: \"Name, Age years, Dept department\"\n",
    "5. Use apply to add 10% bonus to salaries in the IT department only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72420d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 11: Use the employees_full data from Exercise 10\n",
    "print(\"Employee Data:\")\n",
    "print(employees_full)\n",
    "\n",
    "# TODO: Create custom columns using apply and lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3589315",
   "metadata": {},
   "source": [
    "#### Exercise 12: Merging DataFrames\n",
    "\n",
    "**Task:** Combine data from multiple sources about a company.\n",
    "\n",
    "**Requirements:**\n",
    "1. Merge employees with departments (many-to-one)\n",
    "2. Merge the result with salaries (one-to-one)\n",
    "3. Perform an outer join to see which employees don't have salary data\n",
    "4. Use the indicator parameter to track the merge status\n",
    "5. Calculate the average salary by department location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f1e98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 12: Sample data from multiple sources\n",
    "\n",
    "# Employee basic info\n",
    "emp_info = pd.DataFrame({\n",
    "    'EmployeeID': [101, 102, 103, 104, 105, 106],\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve', 'Frank'],\n",
    "    'DepartmentID': [1, 2, 1, 3, 2, 1]\n",
    "})\n",
    "\n",
    "# Department information\n",
    "dept_info = pd.DataFrame({\n",
    "    'DepartmentID': [1, 2, 3, 4],\n",
    "    'Department': ['IT', 'Sales', 'HR', 'Marketing'],\n",
    "    'Location': ['New York', 'Los Angeles', 'Chicago', 'Boston']\n",
    "})\n",
    "\n",
    "# Salary information (incomplete - not all employees)\n",
    "salary_info = pd.DataFrame({\n",
    "    'EmployeeID': [101, 102, 103, 105],\n",
    "    'Salary': [70000, 75000, 90000, 70000],\n",
    "    'Bonus': [5000, 7000, 9000, 7000]\n",
    "})\n",
    "\n",
    "print(\"Employee Info:\")\n",
    "print(emp_info)\n",
    "print(\"\\nDepartment Info:\")\n",
    "print(dept_info)\n",
    "print(\"\\nSalary Info:\")\n",
    "print(salary_info)\n",
    "\n",
    "# TODO: Merge the DataFrames and perform analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274ec15e",
   "metadata": {},
   "source": [
    "#### Exercise 13: Pivot Tables\n",
    "\n",
    "**Task:** Create pivot tables to analyze multi-dimensional sales data.\n",
    "\n",
    "**Requirements:**\n",
    "1. Create a pivot table showing total Sales by Product (rows) and Region (columns)\n",
    "2. Create a pivot table showing average Sales by Month and Product\n",
    "3. Add row and column totals (margins=True)\n",
    "4. Find which Product-Region combination has the highest sales\n",
    "5. Calculate what percentage each region contributes to total sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74871be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 13: Sample multi-dimensional sales data\n",
    "sales_multi = pd.DataFrame({\n",
    "    'Date': pd.date_range('2024-01-01', periods=20, freq='W'),\n",
    "    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Laptop'] * 4,\n",
    "    'Region': ['North', 'North', 'South', 'East', 'West'] * 4,\n",
    "    'Sales': [1200, 45, 120, 680, 1300, 1100, 50, 110, 700, 1250,\n",
    "              1150, 48, 115, 690, 1280, 1220, 52, 125, 710, 1290],\n",
    "    'Units': [2, 3, 4, 2, 2, 2, 4, 3, 2, 2,\n",
    "              2, 3, 3, 2, 2, 2, 4, 4, 2, 2]\n",
    "})\n",
    "\n",
    "print(\"Multi-dimensional Sales Data:\")\n",
    "print(sales_multi.head(10))\n",
    "print(f\"\\n... ({len(sales_multi)} total rows)\")\n",
    "\n",
    "# TODO: Create and analyze pivot tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd048af9",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Pandas is essential for:\n",
    "- Data manipulation and cleaning\n",
    "- Exploratory data analysis\n",
    "- Data preparation for machine learning\n",
    "- Working with structured data\n",
    "\n",
    "**Key Takeaways:**\n",
    "- DataFrame and Series are the core data structures\n",
    "- Always use `.loc[]` or `.iloc[]` for explicit row selection\n",
    "- Powerful selection, filtering, and indexing capabilities\n",
    "- GroupBy enables split-apply-combine operations\n",
    "- Easy handling of missing data and duplicates\n",
    "- Built-in support for merging, joining, and reshaping\n",
    "- Excellent time series functionality\n",
    "- Direct integration with visualization libraries\n",
    "- `apply()` and lambda for custom transformations\n",
    "- Use vectorized operations when possible for better performance\n",
    "\n",
    "**Next Steps:**\n",
    "- Practice with real datasets (Kaggle, UCI ML Repository)\n",
    "- Learn data visualization with Matplotlib and Seaborn\n",
    "- Explore advanced pandas features (MultiIndex, window functions)\n",
    "- Study data cleaning and preprocessing techniques\n",
    "- Combine with NumPy for numerical operations\n",
    "\n",
    "**Additional Resources:**\n",
    "- Pandas Official Tutorial: https://pandas.pydata.org/docs/getting_started/intro_tutorials/\n",
    "- 10 Minutes to Pandas: https://pandas.pydata.org/docs/user_guide/10min.html\n",
    "- Pandas Cheat Sheet: https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf\n",
    "- Pandas API Reference: https://pandas.pydata.org/docs/reference/index.html"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
